//! 2å±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆMLPï¼‰ã«ã‚ˆã‚‹å¤šæ¬¡å…ƒé–¢æ•°ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®ãƒ‡ãƒ¢
//!
//! 2æ¬¡å…ƒå…¥åŠ› (x, y) ã‹ã‚‰è¤‡é›‘ãªé–¢æ•° z = f(x, y) ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
//!
//! ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¢æ•°:
//!   z = sin(Ï€x) * cos(Ï€y) + 0.3xÂ² - 0.2yÂ² + 0.1xy
//!
//! ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ :
//!   å…¥åŠ›å±¤ (2) â†’ éš ã‚Œå±¤ (32) â†’ å‡ºåŠ›å±¤ (1)
//!
//! å®Ÿè¡Œ:
//! ```
//! cargo run --example mlp_fitting -p autograd --features ndarray
//! ```

use autograd::Variable;
use indicatif::{ProgressBar, ProgressStyle};
use ndarray::Array2;
use rand::Rng;

// ============================================================================
// ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¢æ•°
// ============================================================================

/// å­¦ç¿’å¯¾è±¡ã®é–¢æ•°: z = sin(Ï€x) * cos(Ï€y) + 0.3xÂ² - 0.2yÂ² + 0.1xy
fn target_function(x: f64, y: f64) -> f64 {
    let pi = std::f64::consts::PI;
    (pi * x).sin() * (pi * y).cos() + 0.3 * x * x - 0.2 * y * y + 0.1 * x * y
}

// ============================================================================
// ReLU æ´»æ€§åŒ–é–¢æ•°
// ============================================================================

/// ReLU: max(x, 0)
fn relu(x: &Variable<Array2<f64>>) -> Variable<Array2<f64>> {
    x.maximum(&x.zeros_like())
}

// ============================================================================
// 2å±¤ MLP
// ============================================================================

/// 2å±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆãƒã‚¤ã‚¢ã‚¹ãªã—ã€ReLUæ´»æ€§åŒ–ï¼‰
struct Mlp {
    // ç¬¬1å±¤: [å…¥åŠ›, éš ã‚Œ]
    w1: Variable<Array2<f64>>,
    // ç¬¬2å±¤: [éš ã‚Œ, å‡ºåŠ›]
    w2: Variable<Array2<f64>>,
}

impl Mlp {
    fn new(input_dim: usize, hidden_dim: usize, output_dim: usize) -> Self {
        let mut rng = rand::thread_rng();

        // He åˆæœŸåŒ– (ReLU å‘ã‘)
        let scale1 = (2.0 / input_dim as f64).sqrt();
        let scale2 = (2.0 / hidden_dim as f64).sqrt();

        let w1_data: Vec<f64> = (0..input_dim * hidden_dim)
            .map(|_| rng.gen_range(-scale1..scale1))
            .collect();
        let w2_data: Vec<f64> = (0..hidden_dim * output_dim)
            .map(|_| rng.gen_range(-scale2..scale2))
            .collect();

        let w1 = Array2::from_shape_vec((input_dim, hidden_dim), w1_data).unwrap();
        let w2 = Array2::from_shape_vec((hidden_dim, output_dim), w2_data).unwrap();

        Self {
            w1: Variable::new(w1),
            w2: Variable::new(w2),
        }
    }

    /// é †ä¼æ’­: x [batch, input] â†’ y [batch, output]
    fn forward(&self, x: &Variable<Array2<f64>>) -> Variable<Array2<f64>> {
        // ç¬¬1å±¤: z1 = x @ W1
        let z1 = x.matmul(&self.w1);

        // ReLU æ´»æ€§åŒ–: h = max(z1, 0)
        let h = relu(&z1);

        // ç¬¬2å±¤: y = h @ W2
        h.matmul(&self.w2)
    }

    /// å‹¾é…ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–
    fn zero_grad(&self) {
        self.w1.zero_grad();
        self.w2.zero_grad();
    }

    /// å‹¾é…é™ä¸‹æ³•ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
    fn step(&mut self, lr: f64) {
        if let Some(grad) = self.w1.grad() {
            let new_w1 = self.w1.value() - &(grad.value() * lr);
            self.w1 = Variable::new(new_w1);
        }
        if let Some(grad) = self.w2.grad() {
            let new_w2 = self.w2.value() - &(grad.value() * lr);
            self.w2 = Variable::new(new_w2);
        }
    }
}

// ============================================================================
// ãƒ¡ã‚¤ãƒ³
// ============================================================================

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     2å±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã«ã‚ˆã‚‹å¤šæ¬¡å…ƒé–¢æ•°ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° ãƒ‡ãƒ¢       â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();

    // ============================================================
    // 1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    // ============================================================
    println!("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...");

    let mut rng = rand::thread_rng();
    let n_samples = 500;
    let noise_scale = 0.05;

    // [-1, 1] Ã— [-1, 1] ã®ç¯„å›²ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    let mut x_data: Vec<f64> = Vec::with_capacity(n_samples * 2);
    let mut y_data: Vec<f64> = Vec::with_capacity(n_samples);

    for _ in 0..n_samples {
        let x = rng.gen_range(-1.0..1.0);
        let y = rng.gen_range(-1.0..1.0);
        let z = target_function(x, y) + rng.gen_range(-noise_scale..noise_scale);

        x_data.push(x);
        x_data.push(y);
        y_data.push(z);
    }

    let x_train = Array2::from_shape_vec((n_samples, 2), x_data).unwrap();
    let y_train = Array2::from_shape_vec((n_samples, 1), y_data).unwrap();

    println!("   ã‚µãƒ³ãƒ—ãƒ«æ•°: {}", n_samples);
    println!("   å…¥åŠ›æ¬¡å…ƒ: 2 (x, y)");
    println!("   å‡ºåŠ›æ¬¡å…ƒ: 1 (z)");
    println!("   ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¢æ•°: z = sin(Ï€x)cos(Ï€y) + 0.3xÂ² - 0.2yÂ² + 0.1xy");
    println!();

    // ============================================================
    // 2. MLP åˆæœŸåŒ–
    // ============================================================
    println!("ğŸ”§ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–...");

    let input_dim = 2;
    let hidden_dim = 32;
    let output_dim = 1;

    let mut mlp = Mlp::new(input_dim, hidden_dim, output_dim);

    println!(
        "   æ§‹é€ : {} â†’ {} â†’ {} (ãƒã‚¤ã‚¢ã‚¹ãªã—)",
        input_dim, hidden_dim, output_dim
    );
    println!(
        "   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {} (W1: {}Ã—{} + W2: {}Ã—{})",
        input_dim * hidden_dim + hidden_dim * output_dim,
        input_dim,
        hidden_dim,
        hidden_dim,
        output_dim
    );
    println!();

    // ============================================================
    // 3. å­¦ç¿’ï¼ˆè‡ªå‹•å¾®åˆ†ã«ã‚ˆã‚‹å‹¾é…é™ä¸‹æ³•ï¼‰
    // ============================================================
    let epochs = 1000;
    let lr = 0.01;
    let batch_size = 50;
    let n_batches = n_samples / batch_size;

    println!(
        "ğŸš€ å­¦ç¿’é–‹å§‹ (epochs={}, lr={}, batch_size={})",
        epochs, lr, batch_size
    );
    println!("   æ´»æ€§åŒ–é–¢æ•°: ReLU (max(x, 0))");
    println!("   å‹¾é…è¨ˆç®—: è‡ªå‹•å¾®åˆ† (ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³)");
    println!();

    let pb = ProgressBar::new(epochs as u64);
    pb.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} (loss: {msg})")
            .unwrap()
            .progress_chars("â–ˆâ–‰â–Šâ–‹â–Œâ–â–â–  "),
    );

    let mut loss_history: Vec<f64> = Vec::new();

    for epoch in 0..epochs {
        let mut epoch_loss = 0.0;

        for batch_idx in 0..n_batches {
            let start = batch_idx * batch_size;
            let end = start + batch_size;

            let x_batch = x_train.slice(ndarray::s![start..end, ..]).to_owned();
            let y_batch = y_train.slice(ndarray::s![start..end, ..]).to_owned();

            // å‹¾é…ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–
            mlp.zero_grad();

            // é †ä¼æ’­
            let x_var = Variable::new(x_batch);
            let y_var = Variable::new(y_batch);
            let pred = mlp.forward(&x_var);

            // MSEæå¤±: L = mean((pred - target)Â²)
            let diff = &pred - &y_var;
            let squared = &diff * &diff;
            let loss = squared.sum(0).sum(1); // ã‚¹ã‚«ãƒ©ãƒ¼ã«ç¸®ç´„

            // æå¤±å€¤ã‚’è¨˜éŒ²
            let loss_val = loss.value()[[0, 0]] / (batch_size as f64);
            epoch_loss += loss_val;

            // é€†ä¼æ’­
            // å‹¾é…ã‚¹ã‚±ãƒ¼ãƒ«: 1/batch_size (MSEã®å¹³å‡åŒ–)
            let grad_scale = 1.0 / (batch_size as f64);
            let grad = Variable::new(Array2::from_elem((1, 1), grad_scale));
            loss.backward_with(grad);

            // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            mlp.step(lr);
        }

        epoch_loss /= n_batches as f64;
        loss_history.push(epoch_loss);

        if epoch % 10 == 0 || epoch == epochs - 1 {
            pb.set_message(format!("{:.6}", epoch_loss));
        }
        pb.inc(1);
    }

    pb.finish_with_message(format!("{:.6}", loss_history.last().unwrap()));
    println!();

    // ============================================================
    // 4. çµæœè¡¨ç¤º
    // ============================================================
    println!("âœ… å­¦ç¿’å®Œäº†!");
    println!();
    println!("ğŸ“ˆ æœ€çµ‚çµæœ:");
    println!("   æœ€çµ‚æå¤±: {:.6}", loss_history.last().unwrap());
    println!();

    // ãƒ†ã‚¹ãƒˆ: ã„ãã¤ã‹ã®ç‚¹ã§äºˆæ¸¬ã¨çœŸå€¤ã‚’æ¯”è¼ƒ
    println!("ğŸ“Š äºˆæ¸¬ vs çœŸå€¤ (ã‚µãƒ³ãƒ—ãƒ«):");
    println!(
        "   {:>8} {:>8} â”‚ {:>10} {:>10} â”‚ {:>8}",
        "x", "y", "äºˆæ¸¬", "çœŸå€¤", "èª¤å·®"
    );
    println!("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let test_points = [
        (0.0, 0.0),
        (0.5, 0.5),
        (-0.5, 0.5),
        (0.3, -0.7),
        (-0.8, -0.3),
    ];

    for (x, y) in test_points {
        let input = Array2::from_shape_vec((1, 2), vec![x, y]).unwrap();
        let x_var = Variable::new(input);
        let pred = mlp.forward(&x_var);
        let pred_val = pred.value()[[0, 0]];
        let true_val = target_function(x, y);
        let error = (pred_val - true_val).abs();

        println!(
            "   {:>8.3} {:>8.3} â”‚ {:>10.4} {:>10.4} â”‚ {:>8.4}",
            x, y, pred_val, true_val, error
        );
    }
    println!();

    // æå¤±æ¨ç§»ã‚°ãƒ©ãƒ•
    println!("ğŸ“‰ æå¤±ã®æ¨ç§»:");
    use textplots::{Chart, Plot, Shape};

    let loss_points: Vec<(f32, f32)> = loss_history
        .iter()
        .enumerate()
        .step_by(10)
        .map(|(i, &l)| (i as f32, l as f32))
        .collect();

    Chart::new(100, 30, 0.0, epochs as f32)
        .lineplot(&Shape::Lines(&loss_points))
        .nice();

    println!();
    println!("ã“ã®ãƒ‡ãƒ¢ã§ã¯ ReLU æ´»æ€§åŒ–é–¢æ•°ã¨è‡ªå‹•å¾®åˆ†ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚");
}
