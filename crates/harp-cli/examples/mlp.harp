// Multi-Layer Perceptron (MLP) Layer
// Computes: y = ReLU(x @ W1 + b1) @ W2 + b2
//
// Note: ReLU is approximated as max(x, 0)

// Single linear layer with ReLU activation
graph<B=1, I=1, O=1> linear_relu(x: f32[B, I], w: f32[I, O], b: f32[O]) -> (y: f32[B, O]) {
    // Linear: x @ w + b
    linear = matmul(x, w)
    b_exp = b.unsqueeze(0).expand([B, O])
    biased = linear + b_exp

    // ReLU: max(x, 0)
    zero = 0.0
    result = max(biased, zero)
    return result
}

// Two-layer MLP
graph<B=1, D=1, H=1> mlp(
    x: f32[B, D],
    w1: f32[D, H],
    b1: f32[H],
    w2: f32[H, D],
    b2: f32[D]
) -> (y: f32[B, D]) {
    // First layer with ReLU
    h1 = matmul(x, w1)
    b1_exp = b1.unsqueeze(0).expand([B, H])
    h1_biased = h1 + b1_exp
    zero = 0.0
    h1_relu = max(h1_biased, zero)

    // Second layer (no activation for output)
    h2 = matmul(h1_relu, w2)
    b2_exp = b2.unsqueeze(0).expand([B, D])
    result = h2 + b2_exp
    return result
}
