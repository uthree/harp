// Subgraph Demo
// This example demonstrates the subgraph feature where graphs can call
// other graphs as functions.
//
// The 'main' graph is the entry point. All other graphs are subgraphs
// that can be called from main or other subgraphs.

// ReLU activation function: max(x, 0)
graph<B=1, N=1> relu(x: f32[B, N]) -> (y: f32[B, N]) {
    zero = 0.0
    result = max(x, zero)
    return result
}

// Linear layer: y = x @ w + b
graph<B=1, I=1, O=1> linear(x: f32[B, I], w: f32[I, O], b: f32[O]) -> (y: f32[B, O]) {
    xw = matmul(x, w)
    b_exp = b.unsqueeze(0).expand([B, O])
    result = xw + b_exp
    return result
}

// Two-layer MLP using subgraphs
// y = relu(linear(relu(linear(x, w1, b1)), w2, b2))
graph<B=1, D=1, H=1> main(
    x: f32[B, D],
    w1: f32[D, H],
    b1: f32[H],
    w2: f32[H, D],
    b2: f32[D]
) -> (y: f32[B, D]) {
    // First layer: linear + relu
    h1 = linear(x, w1, b1)
    h1_act = relu(h1)

    // Second layer: linear + relu
    h2 = linear(h1_act, w2, b2)
    result = relu(h2)
    return result
}
