// Layer Normalization
// LayerNorm(x) = (x - mean(x)) / sqrt(var(x) + eps) * gamma + beta
//
// Input:
//   x: [Batch, Features] - input tensor (Features must be concrete)
//   gamma: [Features] - scale parameter
//   beta: [Features] - shift parameter
//
// Note: Using concrete feature size (256) for mean computation

graph<B=1> main(x: f32[B, 256], gamma: f32[256], beta: f32[256]) -> (y: f32[B, 256]) {
    // Compute mean along features dimension
    mean = x.mean(dim=1)
    mean_exp = mean.unsqueeze(1).expand([B, 256])

    // Compute variance: E[(x - mean)^2]
    x_centered = x - mean_exp
    x_sq = x_centered.square()
    var = x_sq.mean(dim=1)

    // Add epsilon for numerical stability and compute 1/sqrt(var + eps)
    eps = 0.00001
    var_exp = var.unsqueeze(1).expand([B, 256])
    inv_std = (var_exp + eps).rsqrt()

    // Normalize
    normalized = x_centered * inv_std

    // Scale and shift with gamma and beta
    gamma_exp = gamma.unsqueeze(0).expand([B, 256])
    beta_exp = beta.unsqueeze(0).expand([B, 256])

    result = normalized * gamma_exp + beta_exp
    return result
}
