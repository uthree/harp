// Activation Functions Demo
// Common activation functions used in neural networks

// Main entry point using ReLU
// ReLU: max(x, 0)
graph<N=1, M=1> main(x: f32[N, M]) -> (y: f32[N, M]) {
    zero = 0.0
    result = max(x, zero)
    return result
}

// Sigmoid: 1 / (1 + exp(-x))
// Using exp2: exp(x) = exp2(x * log2(e))
graph<N=1, M=1> sigmoid(x: f32[N, M]) -> (y: f32[N, M]) {
    log2_e = 1.4427
    neg_x = -x
    exp_neg = (neg_x * log2_e).exp2()
    one = 1.0
    result = (one + exp_neg).recip()
    return result
}

// Tanh: (exp(x) - exp(-x)) / (exp(x) + exp(-x))
// Simplified: 2 * sigmoid(2x) - 1
graph<N=1, M=1> tanh(x: f32[N, M]) -> (y: f32[N, M]) {
    two = 2.0
    one = 1.0
    log2_e = 1.4427

    x2 = x * two
    neg_x2 = -x2
    exp_neg = (neg_x2 * log2_e).exp2()
    sigmoid_2x = (one + exp_neg).recip()

    result = two * sigmoid_2x - one
    return result
}

// SiLU / Swish: x * sigmoid(x)
graph<N=1, M=1> silu(x: f32[N, M]) -> (y: f32[N, M]) {
    log2_e = 1.4427
    neg_x = -x
    exp_neg = (neg_x * log2_e).exp2()
    one = 1.0
    sigmoid_x = (one + exp_neg).recip()
    result = x * sigmoid_x
    return result
}

// Squared ReLU: max(x, 0)^2
graph<N=1, M=1> squared_relu(x: f32[N, M]) -> (y: f32[N, M]) {
    zero = 0.0
    relu_x = max(x, zero)
    result = relu_x.square()
    return result
}
